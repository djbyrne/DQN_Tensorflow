{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from vizdoom import *\n",
    "import itertools as it\n",
    "import random\n",
    "from random import sample, randint, random, choice\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import skimage.color, skimage.transform\n",
    "import tensorflow as tf\n",
    "from tqdm import trange\n",
    "from collections import deque\n",
    "from tensorflow.contrib.layers import flatten, conv2d, fully_connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "learning_rate = 0.00025\n",
    "discount_factor = 0.99\n",
    "epochs = 3\n",
    "learning_steps = 2000\n",
    "total_steps = epochs*learning_steps\n",
    "replay_memory_size = 10000\n",
    "batch_size = 32\n",
    "\n",
    "#state settings\n",
    "frame_repeat = 8\n",
    "resolution = (30, 45)\n",
    "stacks = 2\n",
    "steps = 0\n",
    "state_size = [resolution[0], resolution[1],stacks]  \n",
    "stacked_frames  =  deque([np.zeros(resolution, dtype=np.int) for i in range(stacks)], maxlen=stacks) \n",
    "\n",
    "state_dict = {\"frame_repeat\":frame_repeat, \"resolution\":resolution, \"stacks\":stacks,\"state_size\":state_size}\n",
    "\n",
    "#exploration params\n",
    "eps_start = 1.0\n",
    "eps_end = 0.0001\n",
    "observe = 0.1 * total_steps\n",
    "explore = 0.6 * total_steps\n",
    "decay_rate = (explore-observe) * (eps_start - eps_end)\n",
    "\n",
    "model_savefile = \"/tmp/model.ckpt\"\n",
    "save_model = True\n",
    "load_model = False\n",
    "skip_learning = False\n",
    "episodes_to_watch = 10\n",
    "# Configuration file path\n",
    "config_file_path = \"C:/vizdoom/vizdoom115pre/scenarios/simpler_basic.cfg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    img = img[30:-10,30:-30]\n",
    "    img = img/255.0\n",
    "    img = skimage.transform.resize(img,resolution)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "   \n",
    "    if is_new_episode:\n",
    "        \n",
    "        stacked_frames = deque([np.zeros(resolution, dtype=np.int) for i in range(stacks)], maxlen=stacks)\n",
    "        \n",
    "        for i in range(stacks):\n",
    "            stacked_frames.append(state)\n",
    "        \n",
    "    else:\n",
    "        stacked_frames.append(state)\n",
    "\n",
    "    # Build the stacked state (first dimension specifies different frames)\n",
    "    stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(step):\n",
    "    if step < observe:\n",
    "        return eps_start\n",
    "    elif step < explore:\n",
    "        return eps_start - (step - observe) / decay_rate\n",
    "    else:\n",
    "        return eps_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vizdoom(config_file_path):\n",
    "        print(\"Initializing doom...\")\n",
    "        game = DoomGame()\n",
    "        game.load_config(config_file_path)\n",
    "        game.set_window_visible(False)\n",
    "        game.set_mode(Mode.PLAYER)\n",
    "        game.set_screen_format(ScreenFormat.GRAY8)\n",
    "        game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "        game.init()\n",
    "        print(\"Doom initialized.\")\n",
    "        return game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain - Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, session,action_size,state_size):\n",
    "        self.sess = session\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        self.model = self.build()\n",
    "       \n",
    "    def build(self):\n",
    "        with tf.variable_scope(\"main\"):\n",
    "            self.input_state = tf.placeholder(tf.float32, [None] + self.state_size, name=\"State\")\n",
    "            self.target_q = tf.placeholder(tf.float32, [None, self.action_size], name=\"TargetQ\")\n",
    "\n",
    "            self.layer_1 = conv2d(self.input_state, num_outputs=8, kernel_size=(6,6), stride=3,activation_fn=tf.nn.relu, padding='SAME')\n",
    "            tf.summary.histogram('layer_1',self.layer_1)\n",
    "\n",
    "            self.layer_2 = conv2d(self.layer_1, num_outputs=8, kernel_size=(3,3), stride=2,activation_fn=tf.nn.relu, padding='SAME')\n",
    "            tf.summary.histogram('layer_2',self.layer_2)\n",
    "\n",
    "            self.flat = flatten(self.layer_2)\n",
    "\n",
    "            self.fc = fully_connected(self.flat, num_outputs=128)\n",
    "            tf.summary.histogram('fc',self.fc)\n",
    "\n",
    "            self.output = fully_connected(self.fc, num_outputs=self.action_size, activation_fn=None)\n",
    "            tf.summary.histogram('output',self.output)\n",
    "\n",
    "            self.max_q = tf.argmax(self.output, 1)\n",
    "\n",
    "            self.loss = tf.losses.mean_squared_error(self.output, self.target_q)\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "            self.train_step = self.optimizer.minimize(self.loss)\n",
    "    \n",
    "    def learn(self,s1, tq):\n",
    "        feed_dict = {self.input_state: s1, self.target_q: tq}\n",
    "        l, _ = self.sess.run([self.loss, self.train_step], feed_dict=feed_dict)\n",
    "        return l\n",
    "    \n",
    "    def get_q_values(self,state):\n",
    "        return self.sess.run(self.output, feed_dict={self.input_state: state})\n",
    "\n",
    "    def get_best_action(self,state):\n",
    "        return self.sess.run(self.max_q, feed_dict={self.input_state: state})\n",
    "\n",
    "    def simple_get_best_action(self,state):\n",
    "        return self.get_best_action(state.reshape((1, *state.shape)))[0]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory - Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "        self.size = 0\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.size += 1\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def length(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def size(self):\n",
    "        return self.size\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    def __init__(self, game, agent, memory,state_dict):\n",
    "        self.game = game\n",
    "        self.agent = agent\n",
    "        self.memory = memory\n",
    "        self.state_dict = state_dict\n",
    "        self.steps = 0\n",
    "        self.stacked_frames  =  deque([np.zeros(state_dict[\"resolution\"], dtype=np.int\n",
    "                                               ) for i in range(state_dict[\"stacks\"])], maxlen=state_dict[\"stacks\"])   \n",
    "        \n",
    "    def run(self, training_steps=2000, epochs=4):\n",
    "        epoch_scores = []\n",
    "        for e in range(epochs):\n",
    "            print(\"\\nEpoch %d\\n-------\" % (e + 1))\n",
    "            epoch_scores = self.run_epoch(training_steps)\n",
    "            epoch_scores = np.array(epoch_scores)\n",
    "            print(\"Results: mean: %.1fÂ±%.1f,\" % (epoch_scores.mean(), epoch_scores.std()), \\\n",
    "                  \"min: %.1f,\" % epoch_scores.min(), \"max: %.1f,\" % epoch_scores.max())\n",
    "            \n",
    "        self.game.close()\n",
    "            \n",
    "    def run_epoch(self, training_steps):\n",
    "        \n",
    "        self.game.new_episode()\n",
    "        frame = preprocess(game.get_state().screen_buffer)\n",
    "        state, self.stacked_frames = stack_frames(self.stacked_frames, frame, True)\n",
    "        train_scores = []\n",
    "\n",
    "        for i in range(training_steps):\n",
    "\n",
    "            eps = get_epsilon(self.steps)\n",
    "            if random() <= eps:\n",
    "                 a = randint(0, len(actions) - 1)\n",
    "            else:    \n",
    "                a = self.agent.simple_get_best_action(state)\n",
    "\n",
    "            reward = self.game.make_action(actions[a], frame_repeat)\n",
    "\n",
    "            done = self.game.is_episode_finished()\n",
    "\n",
    "            # Get the next state\n",
    "            if done:\n",
    "                next_frame = np.zeros(state_dict[\"resolution\"], dtype=np.int)\n",
    "            else:\n",
    "                next_frame = preprocess(self.game.get_state().screen_buffer)\n",
    "\n",
    "            # Stack the frame of the next_state\n",
    "            next_state, self.stacked_frames = stack_frames(self.stacked_frames, next_frame, False)\n",
    "\n",
    "            # Add experience to memory\n",
    "            memory.add((state, a, reward, next_state, done))\n",
    "\n",
    "            # st+1 is now our current state\n",
    "            state = next_state\n",
    "\n",
    "            if self.steps > observe:\n",
    "                self.learn_from_memory()\n",
    "            self.steps +=1\n",
    "\n",
    "            if done:\n",
    "                train_scores.append(self.game.get_total_reward())\n",
    "                self.game.new_episode()\n",
    "                frame = preprocess(self.game.get_state().screen_buffer)\n",
    "                state, self.stacked_frames = stack_frames(self.stacked_frames, frame, True)\n",
    "                \n",
    "        return train_scores\n",
    "        \n",
    "                \n",
    "    def learn_from_memory(self):\n",
    "        \"\"\" Learns from a single transition (making use of replay memory).\n",
    "        s2 is ignored if s2_isterminal \"\"\"\n",
    "\n",
    "        # Get a random minibatch from the replay memory and learns from it.\n",
    "        batch = memory.sample(batch_size)\n",
    "        states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "        actions_mb = np.array([each[1] for each in batch])\n",
    "        rewards_mb = np.array([each[2] for each in batch]) \n",
    "        next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "        dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "        max_q_values = np.max(self.agent.get_q_values(next_states_mb), axis=1)\n",
    "\n",
    "        target_q = self.agent.get_q_values(states_mb)\n",
    "        target_q[np.arange(target_q.shape[0]), actions_mb] = rewards_mb + discount_factor * (1 - dones_mb) * max_q_values\n",
    "        self.agent.learn(states_mb, target_q)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing doom...\n",
      "Doom initialized.\n",
      "Training....\n",
      "\n",
      "Epoch 1\n",
      "-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djbyrne\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "C:\\Users\\djbyrne\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: -53.0Â±191.0, min: -395.0, max: 95.0,\n",
      "\n",
      "Epoch 2\n",
      "-------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-054d9c94278b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training....\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-0effb15afbe3>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, training_steps, epochs)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nEpoch %d\\n-------\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mepoch_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mepoch_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Results: mean: %.1fÂ±%.1f,\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m                   \u001b[1;34m\"min: %.1f,\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mepoch_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"max: %.1f,\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mepoch_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-0effb15afbe3>\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(self, training_steps)\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_get_best_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_episode_finished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "game = init_vizdoom(config_file_path)\n",
    "    \n",
    "n = game.get_available_buttons_size()\n",
    "actions = [list(a) for a in it.product([0, 1], repeat=n)]\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "agent = DQN(session, len(actions), state_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "memory = Memory(replay_memory_size)\n",
    "env = Environment(game, agent, memory, state_dict)\n",
    "\n",
    "print(\"Training....\")\n",
    "env.run(learning_steps, epochs)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
